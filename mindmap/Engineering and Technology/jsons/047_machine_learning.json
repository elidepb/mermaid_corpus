{
    "diagrams": [
        {
            "id": "047",
            "topic": "Machine Learning",
            "category": "Computer Science",
            "description": "Machine Learning encompasses algorithms, techniques, and frameworks for data-driven pattern recognition and prediction. Learning types include supervised learning with labeled training data for classification and regression, unsupervised learning discovering structure in unlabeled data through clustering and dimensionality reduction, reinforcement learning optimizing sequential decisions through reward signals, semi-supervised combining small labeled with large unlabeled datasets, and self-supervised learning from data itself. Classical algorithms employ linear regression for continuous predictions, logistic regression for binary classification, decision trees for interpretable rules, support vector machines for margin maximization, k-nearest neighbors for instance-based learning, and naive Bayes for probabilistic classification. Deep learning architectures utilize convolutional neural networks for image processing, recurrent neural networks for sequential data, transformers for attention mechanisms, generative adversarial networks for data generation, and autoencoders for representation learning. Model evaluation metrics include accuracy, precision, recall, F1-score, ROC-AUC for classification, mean squared error and R-squared for regression, and silhouette score for clustering. Challenges address overfitting preventing generalization, underfitting lacking model capacity, bias-variance tradeoff balancing complexity, curse of dimensionality with high-dimensional data, and class imbalance with skewed distributions.",
            "diagram_type": "mindmap",
            "mermaid_code": "mindmap\n  root((<b>Machine<br>Learning</b>))\n    <b>Learning Types</b>\n      Supervised learning\n      Unsupervised learning\n      Reinforcement learning\n      Semi-supervised learning\n      Self-supervised learning\n    <b>Classical Algorithms</b>\n      Linear regression\n      Logistic regression\n      Decision trees\n      Support vector machines\n      K-nearest neighbors\n      Naive Bayes\n    <b>Deep Learning</b>\n      Convolutional networks\n      Recurrent networks\n      Transformers\n      GANs\n      Autoencoders\n      Attention mechanisms\n    <b>Evaluation Metrics</b>\n      Accuracy\n      Precision & recall\n      F1-score\n      ROC-AUC\n      MSE & R-squared\n      Confusion matrix\n    <b>Model Challenges</b>\n      Overfitting\n      Underfitting\n      Bias-variance tradeoff\n      Curse of dimensionality\n      Class imbalance\n    <b>Feature Engineering</b>\n      Feature selection\n      Feature extraction\n      Dimensionality reduction\n      Feature scaling\n      Encoding categorical\n    <b>Ensemble Methods</b>\n      Random forests\n      Gradient boosting\n      Bagging\n      Stacking\n      AdaBoost\n    <b>Optimization</b>\n      Gradient descent\n      Stochastic gradient descent\n      Adam optimizer\n      Learning rate scheduling\n      Regularization",
            "complexity": "high",
            "tags": [
                "machine_learning",
                "computer_science"
            ],
            "creation_date": "2025-11-04"
        }
    ]
}