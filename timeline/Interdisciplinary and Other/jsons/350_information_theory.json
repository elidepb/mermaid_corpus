{
    "diagrams": [
        {
            "id": "350",
            "topic": "Information Theory",
            "category": "Mathematics/Computer Science",
            "description": "A timeline showing the historical development of Information Theory.",
            "diagram_type": "timeline",
            "mermaid_code": "timeline\n    title Key Milestones in Information Theory\n    1928 : Hartley measure\n          : Symbol-based logarithmic information metric introduced\n    1948 : Shannon's paper\n          : \"A Mathematical Theory of Communication\" defines entropy and capacity\n    1950 : Source coding theorem\n          : Shannon proves lossless compression limits\n    1956 : Kraft-McMillan inequality\n          : Prefix codes linked to average codeword length\n    1961 : Rate-distortion theory\n          : Shannon formalizes lossy compression trade-offs\n    1973 : Wyner-Ziv coding\n          : Distributed source coding for correlated signals\n    1993 : Turbo codes\n          : Berrou's iterative decoding nears channel capacity in practice\n    1998 : LDPC revival\n          : Gallager's codes reintroduced with modern iterative decoders\n    2009 : Polar codes\n          : ArÄ±kan achieves capacity with low-complexity constructions\n    2020 : Quantum Shannon theory\n          : Advances in entanglement-assisted communication limits",
            "complexity": "high",
            "tags": [
                "information_theory",
                "mathematics",
                "computer_science"
            ],
            "creation_date": "2025-01-27"
        }
    ]
}
