{
    "diagrams": [
        {
            "id": "047",
            "topic": "Machine Learning",
            "category": "Engineering",
            "description": "Machine Learning evolved from statistical theory to the foundation of modern AI systems. Early pattern recognition and statistical learning theory developed in the 1950s-1960s. The perceptron algorithm (1957) by Frank Rosenbaum introduced neural learning. Arthur Samuel coined machine learning (1959) while developing checkers-playing programs. The perceptron limitations paper (1969) by Minsky and Papert caused neural network decline. Expert systems dominated the 1970s-1980s using rule-based approaches. Backpropagation algorithm rediscovered (1986) enabled multi-layer neural network training. Support vector machines emerged (1990s) providing strong theoretical foundations. Random forests (2001) advanced ensemble methods. Deep learning renaissance began with unsupervised pretraining (2006) and GPU acceleration. ImageNet competition (2012) with AlexNet demonstrated deep learning's image recognition superiority. Word embeddings like Word2Vec (2013) revolutionized NLP. Generative adversarial networks (2014) enabled realistic data generation. Attention mechanisms and transformers (2017) transformed sequence modeling. Today's machine learning integrates foundation models, few-shot learning, federated learning, and automated machine learning (AutoML).",
            "diagram_type": "timeline",
            "mermaid_code": "timeline\n    title Machine Learning Evolution\n    1950s-1960s : Early Pattern Recognition\n                : Statistical learning theory\n    1957 : Perceptron Algorithm\n         : Rosenbaum's neural learning\n    1959 : Machine Learning Term\n         : Samuel's checkers program\n    1969 : Perceptron Limitations\n         : Minsky & Papert analysis\n    1986 : Backpropagation Rediscovered\n         : Multi-layer training enabled\n    1990s : Support Vector Machines\n          : Theoretical foundations\n    2001 : Random Forests\n         : Ensemble method advancement\n    2006 : Deep Learning Renaissance\n         : Unsupervised pretraining\n    2012 : AlexNet ImageNet Win\n         : Deep learning superiority\n    2013 : Word2Vec Embeddings\n         : NLP revolution begins\n    2014 : GANs Introduced\n         : Generative models\n    2017 : Attention & Transformers\n         : Sequence modeling transformed\n    2020s : Foundation Models\n          : Few-shot & AutoML integration",
            "complexity": "high",
            "tags": [
                "machine_learning",
                "engineering"
            ],
            "creation_date": "2025-11-10"
        }
    ]
}